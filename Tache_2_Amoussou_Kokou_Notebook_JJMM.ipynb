{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1df6b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9811647",
   "metadata": {},
   "source": [
    "0) Install/Imports (si besoin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "537cb368",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install timm==1.0.8 albumentations==1.4.8 opencv-python==4.10.0.84\n",
    "import os, re, json, math, random\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import timm\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60dceae2",
   "metadata": {},
   "source": [
    "1) Utils & parsing des labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c5f2d7f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_EXTS = {\".jpg\", \".jpeg\", \".png\", \".bmp\", \".webp\", \".JPG\", \".PNG\", \".JPEG\"}\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "# Regex : id + \"_\" + Y(1-2 chiffres) + sexe(M/F) + age(2 chiffres)\n",
    "_AGE_RE = re.compile(r'^(?P<pid>\\d+)_(?P<idx>\\d{1,2})(?P<sex>[MF])(?P<age>\\d{2})$')\n",
    "\n",
    "def parse_from_filename(fname: str):\n",
    "    \"\"\"\n",
    "    fname: nom de fichier avec extension (ex: '0007_04M17.JPG')\n",
    "    return: dict {person_id:int, photo_idx:int, sex:int(M=1,F=0), age:int}\n",
    "    \"\"\"\n",
    "    stem = Path(fname).stem  # retire l'extension\n",
    "    m = _AGE_RE.match(stem)\n",
    "    if not m:\n",
    "        raise ValueError(f\"Nom de fichier non conforme: {fname}\")\n",
    "    d = m.groupdict()\n",
    "    return {\n",
    "        \"person_id\": int(d[\"pid\"]),\n",
    "        \"photo_idx\": int(d[\"idx\"]),\n",
    "        \"sex\": 1 if d[\"sex\"] == \"M\" else 0,\n",
    "        \"age\": int(d[\"age\"]),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "05da1b47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'person_id': 7, 'photo_idx': 4, 'sex': 1, 'age': 17}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parse_from_filename('0007_04M17.JPG')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0c031c",
   "metadata": {},
   "source": [
    "2) Dataset PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cfae7c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgeDataset(Dataset):\n",
    "    def __init__(self, img_dir, items, transform=None):\n",
    "        self.img_dir = Path(img_dir)\n",
    "        self.items = items  # list of dicts: {\"path\": str, \"age\": int, \"sex\": 0/1}\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.items)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        it = self.items[i]\n",
    "        path = self.img_dir / it[\"path\"]\n",
    "        img = Image.open(path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        age = torch.tensor([it[\"age\"]], dtype=torch.float32)\n",
    "        sex = torch.tensor([it[\"sex\"]], dtype=torch.float32)\n",
    "        return img, age, sex, it[\"path\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "789acdb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<PIL.Image.Image image mode=RGB size=180x180>,\n",
       " tensor([16.]),\n",
       " tensor([1.]),\n",
       " '0007_01M16.JPG')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AgeDataset(\"data/train\", [{\"path\": \"0007_01M16.JPG\", \"age\": 16, \"sex\": 1}])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b33156",
   "metadata": {},
   "source": [
    "3) Transforms (train/val/test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5d01ae1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean, std = [0.485,0.456,0.406], [0.229,0.224,0.225]\n",
    "\n",
    "train_tf = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.RandomResizedCrop(224, scale=(0.8,1.0)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ColorJitter(0.1,0.1,0.1,0.05),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean, std),\n",
    "    transforms.RandomErasing(p=0.25)\n",
    "])\n",
    "\n",
    "val_tf = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean, std),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23dc78dc",
   "metadata": {},
   "source": [
    "4) Pr√©paration des listes d‚Äôimages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "53362884",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = \"data/train\"\n",
    "test_dir  = \"data/test\"\n",
    "\n",
    "train_items = []\n",
    "for p in sorted(os.listdir(train_dir)):\n",
    "    if Path(p).suffix.lower() in IMG_EXTS:\n",
    "        meta = parse_from_filename(p)\n",
    "        train_items.append({\"path\": p, \"age\": meta[\"age\"], \"sex\": meta[\"sex\"]})\n",
    "\n",
    "test_items = [{\"path\": p} for p in sorted(os.listdir(test_dir)) if Path(p).suffix.lower() in IMG_EXTS]\n",
    "\n",
    "# Stratification par (bucket_age, sex)\n",
    "def age_bucket(a, width=5):\n",
    "    return a // width\n",
    "\n",
    "y_strat = [f\"{age_bucket(x['age'])}_{x['sex']}\" for x in train_items]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8815a125",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'path': '00013_00M19.JPG', 'age': 19, 'sex': 1},\n",
       " {'path': '00013_01M19.JPG', 'age': 19, 'sex': 1},\n",
       " {'path': '00013_02M19.JPG', 'age': 19, 'sex': 1},\n",
       " {'path': '00022_03M19.JPG', 'age': 19, 'sex': 1},\n",
       " {'path': '00022_04M20.JPG', 'age': 20, 'sex': 1},\n",
       " {'path': '00027_00M30.JPG', 'age': 30, 'sex': 1}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_items[:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "87a702a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'path': '0.JPG'},\n",
       " {'path': '1.JPG'},\n",
       " {'path': '10.JPG'},\n",
       " {'path': '100.JPG'},\n",
       " {'path': '101.JPG'},\n",
       " {'path': '102.JPG'}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_items[:6]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c9dfd6f",
   "metadata": {},
   "source": [
    "5) Mod√®le (backbone timm + t√™te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "da3148ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgeRegressor(nn.Module):\n",
    "    def __init__(self, backbone_name=\"convnext_tiny\", use_sex=True):\n",
    "        super().__init__()\n",
    "        self.backbone = timm.create_model(backbone_name, pretrained=True, num_classes=0, global_pool=\"avg\")\n",
    "        feat_dim = self.backbone.num_features\n",
    "        self.use_sex = use_sex\n",
    "        in_dim = feat_dim + (1 if use_sex else 0)\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(in_dim, 1)\n",
    "        )\n",
    "        # self.head = nn.Sequential(\n",
    "        #     nn.Linear(in_dim, 1024),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.Dropout(0.3),\n",
    "        #     nn.Linear(1024, 512),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.Dropout(0.3),\n",
    "        #     nn.Linear(512, max_age+1)\n",
    "        # )\n",
    "\n",
    "\n",
    "    def forward(self, x, sex=None):\n",
    "        f = self.backbone(x)  # (B, feat_dim)\n",
    "        if self.use_sex and sex is not None:\n",
    "            f = torch.cat([f, sex], dim=1)\n",
    "        out = self.head(f)  # (B, 1)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "523ffa9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import timm\n",
    "\n",
    "class AgeClassifier(nn.Module):\n",
    "    def __init__(self, backbone_name=\"convnext_tiny\", max_age=100, use_sex=True, hidden_dim=512):\n",
    "        super().__init__()\n",
    "        self.backbone = timm.create_model(backbone_name, pretrained=True, num_classes=0, global_pool=\"avg\")\n",
    "        feat_dim = self.backbone.num_features\n",
    "        self.use_sex = use_sex\n",
    "        in_dim = feat_dim + (1 if use_sex else 0)\n",
    "\n",
    "        # tu peux augmenter la profondeur du \"head\"\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(in_dim, hidden_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(hidden_dim, max_age+1)  # classes 0..max_age\n",
    "        )\n",
    "\n",
    "    def forward(self, x, sex=None):\n",
    "        f = self.backbone(x)\n",
    "        if self.use_sex and sex is not None:\n",
    "            f = torch.cat([f, sex], dim=1)\n",
    "        logits = self.head(f)         # (B, 101)\n",
    "        probs = torch.softmax(logits, dim=1)\n",
    "        return logits, probs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "888c61f1",
   "metadata": {},
   "source": [
    "6) Entra√Ænement (1 split ou k-fold)\n",
    "\n",
    "    6.1. boucle d‚Äôentra√Ænement simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b4c96a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.amp import autocast\n",
    "def train_one_epoch(model, loader, optim, scaler, device, criterion, max_grad_norm=1.0):\n",
    "    model.train()\n",
    "    loss_sum, n = 0.0, 0\n",
    "    for imgs, ages, sex, _ in loader:\n",
    "        imgs, ages, sex = imgs.to(device), ages.to(device), sex.to(device)\n",
    "        optim.zero_grad(set_to_none=True)\n",
    "        with autocast(device_type=\"cuda\", dtype=torch.float16):\n",
    "            preds = model(imgs, sex)\n",
    "            loss = criterion(preds, ages)\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.unscale_(optim)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "        scaler.step(optim)\n",
    "        scaler.update()\n",
    "        bs = imgs.size(0)\n",
    "        loss_sum += loss.item() * bs\n",
    "        n += bs\n",
    "    return loss_sum / n\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader, device):\n",
    "    model.eval()\n",
    "    mae_sum, mse_sum, n = 0.0, 0.0, 0\n",
    "    y_true, y_pred = [], []\n",
    "    for imgs, ages, sex, _ in loader:\n",
    "        imgs, ages, sex = imgs.to(device), ages.to(device), sex.to(device)\n",
    "        preds = model(imgs, sex)\n",
    "        preds = preds.clamp(0, 100)  # bornes raisonnables\n",
    "        err = torch.abs(preds - ages)\n",
    "        mae_sum += err.sum().item()\n",
    "        mse_sum += ((preds - ages)**2).sum().item()\n",
    "        n += imgs.size(0)\n",
    "        y_true.extend(ages.squeeze(1).cpu().numpy().tolist())\n",
    "        y_pred.extend(preds.squeeze(1).cpu().numpy().tolist())\n",
    "    mae = mae_sum / n\n",
    "    rmse = math.sqrt(mse_sum / n)\n",
    "    # %Within-k\n",
    "    def within_k(k):\n",
    "        return (np.mean(np.abs(np.array(y_pred)-np.array(y_true)) <= k) * 100.0)\n",
    "    metrics = {\n",
    "        \"MAE\": mae, \"RMSE\": rmse,\n",
    "        \"Within_1(%)\": within_k(1),\n",
    "        \"Within_2(%)\": within_k(2),\n",
    "        \"Within_3(%)\": within_k(3),\n",
    "    }\n",
    "    # m√©triques ‚Äúclassification tol√©rante‚Äù (¬±2 ans)\n",
    "    y_ok_true = np.ones_like(y_true)  # on √©value seulement ‚Äúcorrect/incorrect‚Äù\n",
    "    y_ok_pred = (np.abs(np.array(y_pred)-np.array(y_true)) <= 2).astype(int)\n",
    "    P = precision_score(y_ok_true, y_ok_pred)\n",
    "    R = recall_score(y_ok_true, y_ok_pred)\n",
    "    F1 = f1_score(y_ok_true, y_ok_pred)\n",
    "    metrics.update({\"Prec_tol¬±2\": P, \"Rec_tol¬±2\": R, \"F1_tol¬±2\": F1})\n",
    "    return metrics, (y_true, y_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "934f7a8c",
   "metadata": {},
   "source": [
    "    6.2. entra√Ænement principal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "993b38bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "30803aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install --index-url https://download.pytorch.org/whl/cu124 torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfeed152",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_split.py:811: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] temps=7712.6s | MAE=4.361\n",
      "Epoch 01 | train_loss=6.3055 | MAE=4.361 | RMSE=5.608 | W2=29.5% | F1tol2=0.456\n",
      "[2] temps=2675.9s | MAE=5.025\n",
      "Epoch 02 | train_loss=3.6624 | MAE=5.025 | RMSE=6.589 | W2=30.3% | F1tol2=0.465\n",
      "[3] temps=2351.1s | MAE=3.238\n",
      "Epoch 03 | train_loss=3.2495 | MAE=3.238 | RMSE=4.292 | W2=41.8% | F1tol2=0.589\n",
      "[4] temps=2324.2s | MAE=3.264\n",
      "Epoch 04 | train_loss=2.9837 | MAE=3.264 | RMSE=4.297 | W2=41.2% | F1tol2=0.584\n",
      "[5] temps=2355.2s | MAE=3.071\n",
      "Epoch 05 | train_loss=2.8134 | MAE=3.071 | RMSE=4.075 | W2=44.3% | F1tol2=0.614\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 15\n",
    "LR = 3e-4\n",
    "WD = 1e-4\n",
    "\n",
    "# split simple 80/20\n",
    "idx = np.arange(len(train_items))\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "train_idx, val_idx = next(iter(skf.split(idx, y_strat)))\n",
    "\n",
    "train_list = [train_items[i] for i in train_idx]\n",
    "val_list   = [train_items[i] for i in val_idx]\n",
    "\n",
    "train_ds = AgeDataset(train_dir, train_list, transform=train_tf)\n",
    "val_ds   = AgeDataset(train_dir, val_list, transform=val_tf)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=0, pin_memory=True)\n",
    "val_loader   = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=0, pin_memory=True)\n",
    "\n",
    "model = AgeRegressor(\"convnext_tiny\", use_sex=True).to(device)\n",
    "criterion = nn.SmoothL1Loss(beta=1.0)  # Huber\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WD)\n",
    "scaler = torch.amp.GradScaler()\n",
    "\n",
    "best_mae, best_path = 1e9, \"best_model.pt\"\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    t0 = time.time()\n",
    "    tr_loss = train_one_epoch(model, train_loader, optimizer, scaler, device, criterion)\n",
    "    metrics, _ = evaluate(model, val_loader, device)\n",
    "    dt = time.time()-t0\n",
    "    print(f\"[{epoch}] temps={dt:.1f}s | MAE={metrics['MAE']:.3f}\")\n",
    "    print(f\"Epoch {epoch:02d} | train_loss={tr_loss:.4f} | \"\n",
    "          f\"MAE={metrics['MAE']:.3f} | RMSE={metrics['RMSE']:.3f} | \"\n",
    "          f\"W2={metrics['Within_2(%)']:.1f}% | F1tol2={metrics['F1_tol¬±2']:.3f}\")\n",
    "    if metrics[\"MAE\"] < best_mae:\n",
    "        best_mae = metrics[\"MAE\"]\n",
    "        torch.save(model.state_dict(), best_path)\n",
    "\n",
    "print(\"Best MAE:\", best_mae, \"saved:\", best_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b02eae8f",
   "metadata": {},
   "source": [
    "> ## üîé Lecture des m√©triques\n",
    ">\n",
    "> **MAE (Mean Absolute Error) ‚âà 8.8 ans**\n",
    ">  ‚Üí En moyenne, ton mod√®le se trompe de \\~9 ans.\n",
    ">  Pour une t√¢che d‚Äôestimation d‚Äô√¢ge, c‚Äôest **correct** si ton dataset couvre 0‚Äì100 ans, mais ce n‚Äôest pas encore ‚Äútr√®s bon‚Äù.\n",
    ">  (En recherche, des mod√®les sur gros datasets arrivent plut√¥t entre 3‚Äì5 ans de MAE, mais avec beaucoup plus de donn√©es et souvent du pr√©-entrainement sp√©cifique au visage.)\n",
    ">\n",
    ">* **RMSE ‚âà 10.5 ans**\n",
    ">  ‚Üí Les grosses erreurs p√®sent plus (carr√©). Ton mod√®le fait parfois des erreurs bien plus grandes que 9 ans.\n",
    ">\n",
    ">* **Within 2 ans (W2) = 12.7%**\n",
    ">  ‚Üí Seulement 12.7% des pr√©dictions sont √† ¬±2 ans de l‚Äô√¢ge r√©el.\n",
    ">  C‚Äôest assez faible. Sur des mod√®les plus sp√©cialis√©s visage, on viserait plut√¥t 30‚Äì40% ou plus.\n",
    ">\n",
    ">* **F1 tol ¬±2 = 0.225**\n",
    ">  ‚Üí C‚Äôest coh√©rent avec W2 : ton rappel et ta pr√©cision dans cette tol√©rance restent faibles.\n",
    ">\n",
    ">* **Temps par √©poque**\n",
    ">\n",
    ">  * Epoch 1 : 1967 s (‚âà 33 min) ‚Üí probablement DataLoader lent au d√©marrage.\n",
    ">  * Epoch 2 : 2377 s (‚âà 40 min).\n",
    ">  * Epoch 3 : 916 s (‚âà 15 min).\n",
    ">\n",
    "> ## ‚öñÔ∏è Interpr√©tion\n",
    ">\n",
    ">* Ton mod√®le **apprend bien** (MAE passe de 9.58 ‚Üí 8.79 en 3 √©poques).\n",
    ">* **Mais la performance reste moyenne** (erreur \\~9 ans).\n",
    ">* Le temps par √©poque est **√©lev√©** mais acceptable pour ce volume (\\~40k images).\n",
    ">\n",
    ">## üöÄ Pistes d‚Äôam√©lioration\n",
    ">\n",
    ">1. **Mod√®le (backbone)**\n",
    ">\n",
    ">   * On a test√© `convnext_tiny`.\n",
    ">   * Essayer `resnet50`, `efficientnet_b3`, ou m√™me `swin_tiny` via `timm` : certains sont meilleurs sur visages.\n",
    ">\n",
    ">2. **Formulation de la t√¢che**\n",
    ">\n",
    ">   * On es en **r√©gression directe** ‚Üí souvent moins robuste.\n",
    ">   * Essayer la **classification douce (0‚Äì100 ans, soft labels)** : √ßa stabilise et am√©liore g√©n√©ralement MAE et W2.\n",
    ">\n",
    ">3. **Data augmentation**\n",
    ">\n",
    ">   * Am√©liorer la robustesse : flips, l√©g√®re rotation, color jitter.\n",
    ">   * On avait mis RandomErasing : √ßa peut √™tre violent sur des visages ‚Üí √† tester prudemment.\n",
    ">\n",
    ">4. **Entra√Ænement plus long**\n",
    ">\n",
    ">   * Avec seulement 3 √©poques, on n‚Äôa pas encore converg√©.\n",
    ">   * Tester 15‚Äì20 √©poques (en surveillant MAE val).\n",
    ">\n",
    ">5. **Batch normalization et dropout**\n",
    ">\n",
    ">   * V√©rifier si on a assez de r√©gularisation.\n",
    ">   * Un head un peu plus profond (2‚Äì3 couches) peut aider.\n",
    ">\n",
    ">**Conclusion**\n",
    ">\n",
    "> *‚ÄúApr√®s 3 √©poques, notre mod√®le ConvNeXt Tiny atteint une MAE de \\~8.8 ans. Cela montre que le mod√®le apprend (MAE en baisse), mais les performances restent moyennes : seule \\~12% des pr√©dictions sont dans une tol√©rance de ¬±2 ans. Il y a encore une marge d‚Äôam√©lioration, notamment en testant la formulation classification douce, un backbone plus puissant (EfficientNet/ResNet50), et un entra√Ænement plus long (10‚Äì20 √©poques).‚Äù*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "003d8497",
   "metadata": {},
   "source": [
    "7) Inf√©rence sur le test + export CSV/JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966e79b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  filename  age\n",
      "0    0.JPG   32\n",
      "1    1.JPG   33\n",
      "2   10.JPG   40\n",
      "3  100.JPG   43\n",
      "4  101.JPG   40\n"
     ]
    }
   ],
   "source": [
    "# charge meilleurs poids\n",
    "model = AgeRegressor(\"convnext_tiny\", use_sex=True).to(device)\n",
    "model.load_state_dict(torch.load(\"best_model.pt\", map_location=device))\n",
    "model.eval()\n",
    "\n",
    "test_tf = val_tf  # pas d'augmentation forte √† test\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self, img_dir, items, transform=None):\n",
    "        self.img_dir = Path(img_dir); self.items = items; self.transform = transform\n",
    "    def __len__(self): return len(self.items)\n",
    "    def __getitem__(self, i):\n",
    "        p = self.items[i][\"path\"]\n",
    "        img = Image.open(self.img_dir / p).convert(\"RGB\")\n",
    "        if self.transform: img = self.transform(img)\n",
    "        # sexe inconnu en test ‚Üí on met 0.5 (neutre) ou 0 si non utilis√©\n",
    "        sex = torch.tensor([0.5], dtype=torch.float32)\n",
    "        return img, sex, p\n",
    "\n",
    "test_ds = TestDataset(test_dir, test_items, transform=test_tf)\n",
    "test_loader = DataLoader(test_ds, batch_size=64, shuffle=False, num_workers=0, pin_memory=True)\n",
    "\n",
    "preds, names = [], []\n",
    "with torch.no_grad():\n",
    "    for imgs, sex, ps in test_loader:\n",
    "        imgs, sex = imgs.to(device), sex.to(device)\n",
    "        y = model(imgs, sex).clamp(0, 100)\n",
    "        preds.extend(y.squeeze(1).cpu().numpy().tolist())\n",
    "        names.extend(ps)\n",
    "\n",
    "# si besoin d‚Äôentiers:\n",
    "preds_int = [int(round(x)) for x in preds]\n",
    "\n",
    "# Format d‚Äôexport recommand√©:\n",
    "# CSV avec colonnes: filename, age\n",
    "sub_df = pd.DataFrame({\"filename\": names, \"age\": preds_int})\n",
    "sub_df.to_csv(\"submission_age.csv\", index=False)\n",
    "\n",
    "# JSON possible\n",
    "sub_df.to_json(\"submission_age.json\", orient=\"records\", lines=False)\n",
    "print(sub_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa7f5148",
   "metadata": {},
   "source": [
    "8) √âvaluation robustesse (corruptions rapides)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9cf5f21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE sous bright+50%: 10.586\n"
     ]
    }
   ],
   "source": [
    "from torchvision.transforms.functional import adjust_brightness\n",
    "@torch.no_grad()\n",
    "def eval_brightness(model, loader, device, factor=1.5):\n",
    "    model.eval()\n",
    "    mae_sum, n = 0.0, 0\n",
    "    for imgs, ages, sex, _ in loader:\n",
    "        imgs_b = torch.stack([adjust_brightness(img, factor) for img in imgs])\n",
    "        imgs_b, ages, sex = imgs_b.to(device), ages.to(device), sex.to(device)\n",
    "        y = model(imgs_b, sex).clamp(0,100)\n",
    "        mae_sum += torch.abs(y-ages).sum().item()\n",
    "        n += imgs.size(0)\n",
    "    return mae_sum/n\n",
    "\n",
    "b_mae = eval_brightness(model, val_loader, device, factor=1.5)\n",
    "print(\"MAE sous bright+50%:\", round(b_mae,3))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
