{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1df6b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9811647",
   "metadata": {},
   "source": [
    "0) Install/Imports (si besoin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "537cb368",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install timm==1.0.8 albumentations==1.4.8 opencv-python==4.10.0.84\n",
    "import os, re, json, math, random\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import timm\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60dceae2",
   "metadata": {},
   "source": [
    "1) Utils & parsing des labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c5f2d7f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_EXTS = {\".jpg\", \".jpeg\", \".png\", \".bmp\", \".webp\", \".JPG\", \".PNG\", \".JPEG\"}\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "# Regex : id + \"_\" + Y(1-2 chiffres) + sexe(M/F) + age(2 chiffres)\n",
    "_AGE_RE = re.compile(r'^(?P<pid>\\d+)_(?P<idx>\\d{1,2})(?P<sex>[MF])(?P<age>\\d{2})$')\n",
    "\n",
    "def parse_from_filename(fname: str):\n",
    "    \"\"\"\n",
    "    fname: nom de fichier avec extension (ex: '0007_04M17.JPG')\n",
    "    return: dict {person_id:int, photo_idx:int, sex:int(M=1,F=0), age:int}\n",
    "    \"\"\"\n",
    "    stem = Path(fname).stem  # retire l'extension\n",
    "    m = _AGE_RE.match(stem)\n",
    "    if not m:\n",
    "        raise ValueError(f\"Nom de fichier non conforme: {fname}\")\n",
    "    d = m.groupdict()\n",
    "    return {\n",
    "        \"person_id\": int(d[\"pid\"]),\n",
    "        \"photo_idx\": int(d[\"idx\"]),\n",
    "        \"sex\": 1 if d[\"sex\"] == \"M\" else 0,\n",
    "        \"age\": int(d[\"age\"]),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "05da1b47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'person_id': 7, 'photo_idx': 4, 'sex': 1, 'age': 17}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parse_from_filename('0007_04M17.JPG')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0c031c",
   "metadata": {},
   "source": [
    "2) Dataset PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cfae7c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgeDataset(Dataset):\n",
    "    def __init__(self, img_dir, items, transform=None):\n",
    "        self.img_dir = Path(img_dir)\n",
    "        self.items = items  # list of dicts: {\"path\": str, \"age\": int, \"sex\": 0/1}\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.items)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        it = self.items[i]\n",
    "        path = self.img_dir / it[\"path\"]\n",
    "        img = Image.open(path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        age = torch.tensor([it[\"age\"]], dtype=torch.float32)\n",
    "        sex = torch.tensor([it[\"sex\"]], dtype=torch.float32)\n",
    "        return img, age, sex, it[\"path\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b33156",
   "metadata": {},
   "source": [
    "3) Transforms (train/val/test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d01ae1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean, std = [0.485,0.456,0.406], [0.229,0.224,0.225]\n",
    "\n",
    "train_tf = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.RandomResizedCrop(224, scale=(0.8,1.0)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ColorJitter(0.1,0.1,0.1,0.05),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean, std),\n",
    "    transforms.RandomErasing(p=0.25)\n",
    "])\n",
    "\n",
    "val_tf = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean, std),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23dc78dc",
   "metadata": {},
   "source": [
    "4) Préparation des listes d’images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "53362884",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = \"data/train\"  # à adapter\n",
    "test_dir  = \"data/test\"\n",
    "\n",
    "train_items = []\n",
    "for p in sorted(os.listdir(train_dir)):\n",
    "    if Path(p).suffix.lower() in IMG_EXTS:\n",
    "        meta = parse_from_filename(p)\n",
    "        train_items.append({\"path\": p, \"age\": meta[\"age\"], \"sex\": meta[\"sex\"]})\n",
    "\n",
    "test_items = [{\"path\": p} for p in sorted(os.listdir(test_dir)) if Path(p).suffix.lower() in IMG_EXTS]\n",
    "\n",
    "# Stratification par (bucket_age, sex)\n",
    "def age_bucket(a, width=5):\n",
    "    return a // width\n",
    "\n",
    "y_strat = [f\"{age_bucket(x['age'])}_{x['sex']}\" for x in train_items]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8815a125",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'path': '00013_00M19.JPG', 'age': 19, 'sex': 1},\n",
       " {'path': '00013_01M19.JPG', 'age': 19, 'sex': 1},\n",
       " {'path': '00013_02M19.JPG', 'age': 19, 'sex': 1},\n",
       " {'path': '00022_03M19.JPG', 'age': 19, 'sex': 1},\n",
       " {'path': '00022_04M20.JPG', 'age': 20, 'sex': 1},\n",
       " {'path': '00027_00M30.JPG', 'age': 30, 'sex': 1}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_items[:6]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c9dfd6f",
   "metadata": {},
   "source": [
    "5) Modèle (backbone timm + tête)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da3148ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgeRegressor(nn.Module):\n",
    "    def __init__(self, backbone_name=\"convnext_tiny\", use_sex=True):\n",
    "        super().__init__()\n",
    "        self.backbone = timm.create_model(backbone_name, pretrained=True, num_classes=0, global_pool=\"avg\")\n",
    "        feat_dim = self.backbone.num_features\n",
    "        self.use_sex = use_sex\n",
    "        in_dim = feat_dim + (1 if use_sex else 0)\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(in_dim, 1)\n",
    "        )\n",
    "        # self.head = nn.Sequential(\n",
    "        #     nn.Linear(in_dim, 1024),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.Dropout(0.3),\n",
    "        #     nn.Linear(1024, 512),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.Dropout(0.3),\n",
    "        #     nn.Linear(512, max_age+1)\n",
    "        # )\n",
    "\n",
    "\n",
    "    def forward(self, x, sex=None):\n",
    "        f = self.backbone(x)  # (B, feat_dim)\n",
    "        if self.use_sex and sex is not None:\n",
    "            f = torch.cat([f, sex], dim=1)\n",
    "        out = self.head(f)  # (B, 1)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "523ffa9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import timm\n",
    "\n",
    "class AgeClassifier(nn.Module):\n",
    "    def __init__(self, backbone_name=\"convnext_tiny\", max_age=100, use_sex=True, hidden_dim=512):\n",
    "        super().__init__()\n",
    "        self.backbone = timm.create_model(backbone_name, pretrained=True, num_classes=0, global_pool=\"avg\")\n",
    "        feat_dim = self.backbone.num_features\n",
    "        self.use_sex = use_sex\n",
    "        in_dim = feat_dim + (1 if use_sex else 0)\n",
    "\n",
    "        # tu peux augmenter la profondeur du \"head\"\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(in_dim, hidden_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(hidden_dim, max_age+1)  # classes 0..max_age\n",
    "        )\n",
    "\n",
    "    def forward(self, x, sex=None):\n",
    "        f = self.backbone(x)\n",
    "        if self.use_sex and sex is not None:\n",
    "            f = torch.cat([f, sex], dim=1)\n",
    "        logits = self.head(f)         # (B, 101)\n",
    "        probs = torch.softmax(logits, dim=1)\n",
    "        return logits, probs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "888c61f1",
   "metadata": {},
   "source": [
    "6) Entraînement (1 split ou k-fold)\n",
    "\n",
    "    6.1. boucle d’entraînement simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b4c96a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, loader, optim, scaler, device, criterion, max_grad_norm=1.0):\n",
    "    model.train()\n",
    "    loss_sum, n = 0.0, 0\n",
    "    for imgs, ages, sex, _ in loader:\n",
    "        imgs, ages, sex = imgs.to(device), ages.to(device), sex.to(device)\n",
    "        optim.zero_grad(set_to_none=True)\n",
    "        with torch.cuda.amp.autocast(enabled=True):\n",
    "            preds = model(imgs, sex)\n",
    "            loss = criterion(preds, ages)\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.unscale_(optim)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "        scaler.step(optim)\n",
    "        scaler.update()\n",
    "        bs = imgs.size(0)\n",
    "        loss_sum += loss.item() * bs\n",
    "        n += bs\n",
    "    return loss_sum / n\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader, device):\n",
    "    model.eval()\n",
    "    mae_sum, mse_sum, n = 0.0, 0.0, 0\n",
    "    y_true, y_pred = [], []\n",
    "    for imgs, ages, sex, _ in loader:\n",
    "        imgs, ages, sex = imgs.to(device), ages.to(device), sex.to(device)\n",
    "        preds = model(imgs, sex)\n",
    "        preds = preds.clamp(0, 100)  # bornes raisonnables\n",
    "        err = torch.abs(preds - ages)\n",
    "        mae_sum += err.sum().item()\n",
    "        mse_sum += ((preds - ages)**2).sum().item()\n",
    "        n += imgs.size(0)\n",
    "        y_true.extend(ages.squeeze(1).cpu().numpy().tolist())\n",
    "        y_pred.extend(preds.squeeze(1).cpu().numpy().tolist())\n",
    "    mae = mae_sum / n\n",
    "    rmse = math.sqrt(mse_sum / n)\n",
    "    # %Within-k\n",
    "    def within_k(k):\n",
    "        return (np.mean(np.abs(np.array(y_pred)-np.array(y_true)) <= k) * 100.0)\n",
    "    metrics = {\n",
    "        \"MAE\": mae, \"RMSE\": rmse,\n",
    "        \"Within_1(%)\": within_k(1),\n",
    "        \"Within_2(%)\": within_k(2),\n",
    "        \"Within_3(%)\": within_k(3),\n",
    "    }\n",
    "    # métriques “classification tolérante” (±2 ans)\n",
    "    y_ok_true = np.ones_like(y_true)  # on évalue seulement “correct/incorrect”\n",
    "    y_ok_pred = (np.abs(np.array(y_pred)-np.array(y_true)) <= 2).astype(int)\n",
    "    P = precision_score(y_ok_true, y_ok_pred)\n",
    "    R = recall_score(y_ok_true, y_ok_pred)\n",
    "    F1 = f1_score(y_ok_true, y_ok_pred)\n",
    "    metrics.update({\"Prec_tol±2\": P, \"Rec_tol±2\": R, \"F1_tol±2\": F1})\n",
    "    return metrics, (y_true, y_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "934f7a8c",
   "metadata": {},
   "source": [
    "    6.2. entraînement principal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "993b38bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30803aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install --index-url https://download.pytorch.org/whl/cu124 torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfeed152",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_split.py:811: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "C:\\Users\\kokou\\AppData\\Local\\Temp\\ipykernel_33076\\2232029976.py:24: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler()\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 20\n",
    "LR = 3e-4\n",
    "WD = 1e-4\n",
    "\n",
    "# split simple 80/20\n",
    "idx = np.arange(len(train_items))\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "train_idx, val_idx = next(iter(skf.split(idx, y_strat)))\n",
    "\n",
    "train_list = [train_items[i] for i in train_idx]\n",
    "val_list   = [train_items[i] for i in val_idx]\n",
    "\n",
    "train_ds = AgeDataset(train_dir, train_list, transform=train_tf)\n",
    "val_ds   = AgeDataset(train_dir, val_list, transform=val_tf)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, pin_memory=True)\n",
    "val_loader   = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "model = AgeRegressor(\"convnext_tiny\", use_sex=True).to(device)\n",
    "criterion = nn.SmoothL1Loss(beta=1.0)  # Huber\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WD)\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "best_mae, best_path = 1e9, \"best_model.pt\"\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    tr_loss = train_one_epoch(model, train_loader, optimizer, scaler, device, criterion)\n",
    "    metrics, _ = evaluate(model, val_loader, device)\n",
    "    print(f\"Epoch {epoch:02d} | train_loss={tr_loss:.4f} | \"\n",
    "          f\"MAE={metrics['MAE']:.3f} | RMSE={metrics['RMSE']:.3f} | \"\n",
    "          f\"W2={metrics['Within_2(%)']:.1f}% | F1tol2={metrics['F1_tol±2']:.3f}\")\n",
    "    if metrics[\"MAE\"] < best_mae:\n",
    "        best_mae = metrics[\"MAE\"]\n",
    "        torch.save(model.state_dict(), best_path)\n",
    "\n",
    "print(\"Best MAE:\", best_mae, \"saved:\", best_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "003d8497",
   "metadata": {},
   "source": [
    "7) Inférence sur le test + export CSV/JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966e79b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# charge meilleurs poids\n",
    "model = AgeRegressor(\"convnext_tiny\", use_sex=True).to(device)\n",
    "model.load_state_dict(torch.load(\"best_model.pt\", map_location=device))\n",
    "model.eval()\n",
    "\n",
    "test_tf = val_tf  # pas d'augmentation forte à test\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self, img_dir, items, transform=None):\n",
    "        self.img_dir = Path(img_dir); self.items = items; self.transform = transform\n",
    "    def __len__(self): return len(self.items)\n",
    "    def __getitem__(self, i):\n",
    "        p = self.items[i][\"path\"]\n",
    "        img = Image.open(self.img_dir / p).convert(\"RGB\")\n",
    "        if self.transform: img = self.transform(img)\n",
    "        # sexe inconnu en test → on met 0.5 (neutre) ou 0 si non utilisé\n",
    "        sex = torch.tensor([0.5], dtype=torch.float32)\n",
    "        return img, sex, p\n",
    "\n",
    "test_ds = TestDataset(test_dir, test_items, transform=test_tf)\n",
    "test_loader = DataLoader(test_ds, batch_size=64, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "preds, names = [], []\n",
    "with torch.no_grad():\n",
    "    for imgs, sex, ps in test_loader:\n",
    "        imgs, sex = imgs.to(device), sex.to(device)\n",
    "        y = model(imgs, sex).clamp(0, 100)\n",
    "        preds.extend(y.squeeze(1).cpu().numpy().tolist())\n",
    "        names.extend(ps)\n",
    "\n",
    "# si besoin d’entiers:\n",
    "preds_int = [int(round(x)) for x in preds]\n",
    "\n",
    "# Format d’export recommandé:\n",
    "# CSV avec colonnes: filename, age\n",
    "sub_df = pd.DataFrame({\"filename\": names, \"age\": preds_int})\n",
    "sub_df.to_csv(\"submission_age.csv\", index=False)\n",
    "\n",
    "# JSON possible\n",
    "sub_df.to_json(\"submission_age.json\", orient=\"records\", lines=False)\n",
    "print(sub_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa7f5148",
   "metadata": {},
   "source": [
    "8) Évaluation robustesse (corruptions rapides)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9cf5f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms.functional import adjust_brightness\n",
    "@torch.no_grad()\n",
    "def eval_brightness(model, loader, device, factor=1.5):\n",
    "    model.eval()\n",
    "    mae_sum, n = 0.0, 0\n",
    "    for imgs, ages, sex, _ in loader:\n",
    "        imgs_b = torch.stack([adjust_brightness(img, factor) for img in imgs])\n",
    "        imgs_b, ages, sex = imgs_b.to(device), ages.to(device), sex.to(device)\n",
    "        y = model(imgs_b, sex).clamp(0,100)\n",
    "        mae_sum += torch.abs(y-ages).sum().item()\n",
    "        n += imgs.size(0)\n",
    "    return mae_sum/n\n",
    "\n",
    "b_mae = eval_brightness(model, val_loader, device, factor=1.5)\n",
    "print(\"MAE sous bright+50%:\", round(b_mae,3))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
