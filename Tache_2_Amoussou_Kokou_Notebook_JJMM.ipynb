{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1df6b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9811647",
   "metadata": {},
   "source": [
    "0) Install/Imports (si besoin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "537cb368",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install timm==1.0.8 albumentations==1.4.8 opencv-python==4.10.0.84\n",
    "import os, re, json, math, random\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import timm\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60dceae2",
   "metadata": {},
   "source": [
    "1) Utils & parsing des labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c5f2d7f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_EXTS = {\".jpg\", \".jpeg\", \".png\", \".bmp\", \".webp\", \".JPG\", \".PNG\", \".JPEG\"}\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "# Regex : id + \"_\" + Y(1-2 chiffres) + sexe(M/F) + age(2 chiffres)\n",
    "_AGE_RE = re.compile(r'^(?P<pid>\\d+)_(?P<idx>\\d{1,2})(?P<sex>[MF])(?P<age>\\d{2})$')\n",
    "\n",
    "def parse_from_filename(fname: str):\n",
    "    \"\"\"\n",
    "    fname: nom de fichier avec extension (ex: '0007_04M17.JPG')\n",
    "    return: dict {person_id:int, photo_idx:int, sex:int(M=1,F=0), age:int}\n",
    "    \"\"\"\n",
    "    stem = Path(fname).stem  # retire l'extension\n",
    "    m = _AGE_RE.match(stem)\n",
    "    if not m:\n",
    "        raise ValueError(f\"Nom de fichier non conforme: {fname}\")\n",
    "    d = m.groupdict()\n",
    "    return {\n",
    "        \"person_id\": int(d[\"pid\"]),\n",
    "        \"photo_idx\": int(d[\"idx\"]),\n",
    "        \"sex\": 1 if d[\"sex\"] == \"M\" else 0,\n",
    "        \"age\": int(d[\"age\"]),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "05da1b47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'person_id': 7, 'photo_idx': 4, 'sex': 1, 'age': 17}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parse_from_filename('0007_04M17.JPG')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0c031c",
   "metadata": {},
   "source": [
    "2) Dataset PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cfae7c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgeDataset(Dataset):\n",
    "    def __init__(self, img_dir, items, transform=None):\n",
    "        self.img_dir = Path(img_dir)\n",
    "        self.items = items  # list of dicts: {\"path\": str, \"age\": int, \"sex\": 0/1}\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.items)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        it = self.items[i]\n",
    "        path = self.img_dir / it[\"path\"]\n",
    "        img = Image.open(path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        age = torch.tensor([it[\"age\"]], dtype=torch.float32)\n",
    "        sex = torch.tensor([it[\"sex\"]], dtype=torch.float32)\n",
    "        return img, age, sex, it[\"path\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "789acdb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<PIL.Image.Image image mode=RGB size=180x180>,\n",
       " tensor([16.]),\n",
       " tensor([1.]),\n",
       " '0007_01M16.JPG')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AgeDataset(\"data/train\", [{\"path\": \"0007_01M16.JPG\", \"age\": 16, \"sex\": 1}])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b33156",
   "metadata": {},
   "source": [
    "3) Transforms (train/val/test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5d01ae1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean, std = [0.485,0.456,0.406], [0.229,0.224,0.225]\n",
    "\n",
    "train_tf = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.RandomResizedCrop(224, scale=(0.8,1.0)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ColorJitter(0.1,0.1,0.1,0.05),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean, std),\n",
    "    transforms.RandomErasing(p=0.25)\n",
    "])\n",
    "\n",
    "val_tf = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean, std),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23dc78dc",
   "metadata": {},
   "source": [
    "4) Préparation des listes d’images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "53362884",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = \"data/train\"\n",
    "test_dir  = \"data/test\"\n",
    "\n",
    "train_items = []\n",
    "for p in sorted(os.listdir(train_dir)):\n",
    "    if Path(p).suffix.lower() in IMG_EXTS:\n",
    "        meta = parse_from_filename(p)\n",
    "        train_items.append({\"path\": p, \"age\": meta[\"age\"], \"sex\": meta[\"sex\"]})\n",
    "\n",
    "test_items = [{\"path\": p} for p in sorted(os.listdir(test_dir)) if Path(p).suffix.lower() in IMG_EXTS]\n",
    "\n",
    "# Stratification par (bucket_age, sex)\n",
    "def age_bucket(a, width=5):\n",
    "    return a // width\n",
    "\n",
    "y_strat = [f\"{age_bucket(x['age'])}_{x['sex']}\" for x in train_items]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8815a125",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'path': '00013_00M19.JPG', 'age': 19, 'sex': 1},\n",
       " {'path': '00013_01M19.JPG', 'age': 19, 'sex': 1},\n",
       " {'path': '00013_02M19.JPG', 'age': 19, 'sex': 1},\n",
       " {'path': '00022_03M19.JPG', 'age': 19, 'sex': 1},\n",
       " {'path': '00022_04M20.JPG', 'age': 20, 'sex': 1},\n",
       " {'path': '00027_00M30.JPG', 'age': 30, 'sex': 1}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_items[:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "87a702a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'path': '0.JPG'},\n",
       " {'path': '1.JPG'},\n",
       " {'path': '10.JPG'},\n",
       " {'path': '100.JPG'},\n",
       " {'path': '101.JPG'},\n",
       " {'path': '102.JPG'}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_items[:6]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c9dfd6f",
   "metadata": {},
   "source": [
    "5) Modèle (backbone timm + tête)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "da3148ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgeRegressor(nn.Module):\n",
    "    def __init__(self, backbone_name=\"convnext_tiny\", use_sex=True):\n",
    "        super().__init__()\n",
    "        self.backbone = timm.create_model(backbone_name, pretrained=True, num_classes=0, global_pool=\"avg\")\n",
    "        feat_dim = self.backbone.num_features\n",
    "        self.use_sex = use_sex\n",
    "        in_dim = feat_dim + (1 if use_sex else 0)\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(in_dim, 1)\n",
    "        )\n",
    "        # self.head = nn.Sequential(\n",
    "        #     nn.Linear(in_dim, 1024),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.Dropout(0.3),\n",
    "        #     nn.Linear(1024, 512),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.Dropout(0.3),\n",
    "        #     nn.Linear(512, max_age+1)\n",
    "        # )\n",
    "\n",
    "\n",
    "    def forward(self, x, sex=None):\n",
    "        f = self.backbone(x)  # (B, feat_dim)\n",
    "        if self.use_sex and sex is not None:\n",
    "            f = torch.cat([f, sex], dim=1)\n",
    "        out = self.head(f)  # (B, 1)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "523ffa9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import timm\n",
    "\n",
    "class AgeClassifier(nn.Module):\n",
    "    def __init__(self, backbone_name=\"convnext_tiny\", max_age=100, use_sex=True, hidden_dim=512):\n",
    "        super().__init__()\n",
    "        self.backbone = timm.create_model(backbone_name, pretrained=True, num_classes=0, global_pool=\"avg\")\n",
    "        feat_dim = self.backbone.num_features\n",
    "        self.use_sex = use_sex\n",
    "        in_dim = feat_dim + (1 if use_sex else 0)\n",
    "\n",
    "        # tu peux augmenter la profondeur du \"head\"\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(in_dim, hidden_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(hidden_dim, max_age+1)  # classes 0..max_age\n",
    "        )\n",
    "\n",
    "    def forward(self, x, sex=None):\n",
    "        f = self.backbone(x)\n",
    "        if self.use_sex and sex is not None:\n",
    "            f = torch.cat([f, sex], dim=1)\n",
    "        logits = self.head(f)         # (B, 101)\n",
    "        probs = torch.softmax(logits, dim=1)\n",
    "        return logits, probs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "888c61f1",
   "metadata": {},
   "source": [
    "6) Entraînement (1 split ou k-fold)\n",
    "\n",
    "    6.1. boucle d’entraînement simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b4c96a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.amp import autocast\n",
    "def train_one_epoch(model, loader, optim, scaler, device, criterion, max_grad_norm=1.0):\n",
    "    model.train()\n",
    "    loss_sum, n = 0.0, 0\n",
    "    for imgs, ages, sex, _ in loader:\n",
    "        imgs, ages, sex = imgs.to(device), ages.to(device), sex.to(device)\n",
    "        optim.zero_grad(set_to_none=True)\n",
    "        with autocast(device_type=\"cuda\", dtype=torch.float16):\n",
    "            preds = model(imgs, sex)\n",
    "            loss = criterion(preds, ages)\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.unscale_(optim)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "        scaler.step(optim)\n",
    "        scaler.update()\n",
    "        bs = imgs.size(0)\n",
    "        loss_sum += loss.item() * bs\n",
    "        n += bs\n",
    "    return loss_sum / n\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader, device):\n",
    "    model.eval()\n",
    "    mae_sum, mse_sum, n = 0.0, 0.0, 0\n",
    "    y_true, y_pred = [], []\n",
    "    for imgs, ages, sex, _ in loader:\n",
    "        imgs, ages, sex = imgs.to(device), ages.to(device), sex.to(device)\n",
    "        preds = model(imgs, sex)\n",
    "        preds = preds.clamp(0, 100)  # bornes raisonnables\n",
    "        err = torch.abs(preds - ages)\n",
    "        mae_sum += err.sum().item()\n",
    "        mse_sum += ((preds - ages)**2).sum().item()\n",
    "        n += imgs.size(0)\n",
    "        y_true.extend(ages.squeeze(1).cpu().numpy().tolist())\n",
    "        y_pred.extend(preds.squeeze(1).cpu().numpy().tolist())\n",
    "    mae = mae_sum / n\n",
    "    rmse = math.sqrt(mse_sum / n)\n",
    "    # %Within-k\n",
    "    def within_k(k):\n",
    "        return (np.mean(np.abs(np.array(y_pred)-np.array(y_true)) <= k) * 100.0)\n",
    "    metrics = {\n",
    "        \"MAE\": mae, \"RMSE\": rmse,\n",
    "        \"Within_1(%)\": within_k(1),\n",
    "        \"Within_2(%)\": within_k(2),\n",
    "        \"Within_3(%)\": within_k(3),\n",
    "    }\n",
    "    # métriques “classification tolérante” (±2 ans)\n",
    "    y_ok_true = np.ones_like(y_true)  # on évalue seulement “correct/incorrect”\n",
    "    y_ok_pred = (np.abs(np.array(y_pred)-np.array(y_true)) <= 2).astype(int)\n",
    "    P = precision_score(y_ok_true, y_ok_pred)\n",
    "    R = recall_score(y_ok_true, y_ok_pred)\n",
    "    F1 = f1_score(y_ok_true, y_ok_pred)\n",
    "    metrics.update({\"Prec_tol±2\": P, \"Rec_tol±2\": R, \"F1_tol±2\": F1})\n",
    "    return metrics, (y_true, y_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "934f7a8c",
   "metadata": {},
   "source": [
    "    6.2. entraînement principal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "993b38bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "30803aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install --index-url https://download.pytorch.org/whl/cu124 torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfeed152",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_split.py:811: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] temps=7712.6s | MAE=4.361\n",
      "Epoch 01 | train_loss=6.3055 | MAE=4.361 | RMSE=5.608 | W2=29.5% | F1tol2=0.456\n",
      "[2] temps=2675.9s | MAE=5.025\n",
      "Epoch 02 | train_loss=3.6624 | MAE=5.025 | RMSE=6.589 | W2=30.3% | F1tol2=0.465\n",
      "[3] temps=2351.1s | MAE=3.238\n",
      "Epoch 03 | train_loss=3.2495 | MAE=3.238 | RMSE=4.292 | W2=41.8% | F1tol2=0.589\n",
      "[4] temps=2324.2s | MAE=3.264\n",
      "Epoch 04 | train_loss=2.9837 | MAE=3.264 | RMSE=4.297 | W2=41.2% | F1tol2=0.584\n",
      "[5] temps=2355.2s | MAE=3.071\n",
      "Epoch 05 | train_loss=2.8134 | MAE=3.071 | RMSE=4.075 | W2=44.3% | F1tol2=0.614\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 15\n",
    "LR = 3e-4\n",
    "WD = 1e-4\n",
    "\n",
    "# split simple 80/20\n",
    "idx = np.arange(len(train_items))\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "train_idx, val_idx = next(iter(skf.split(idx, y_strat)))\n",
    "\n",
    "train_list = [train_items[i] for i in train_idx]\n",
    "val_list   = [train_items[i] for i in val_idx]\n",
    "\n",
    "train_ds = AgeDataset(train_dir, train_list, transform=train_tf)\n",
    "val_ds   = AgeDataset(train_dir, val_list, transform=val_tf)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=0, pin_memory=True)\n",
    "val_loader   = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=0, pin_memory=True)\n",
    "\n",
    "model = AgeRegressor(\"convnext_tiny\", use_sex=True).to(device)\n",
    "criterion = nn.SmoothL1Loss(beta=1.0)  # Huber\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WD)\n",
    "scaler = torch.amp.GradScaler()\n",
    "\n",
    "best_mae, best_path = 1e9, \"best_model.pt\"\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    t0 = time.time()\n",
    "    tr_loss = train_one_epoch(model, train_loader, optimizer, scaler, device, criterion)\n",
    "    metrics, _ = evaluate(model, val_loader, device)\n",
    "    dt = time.time()-t0\n",
    "    print(f\"[{epoch}] temps={dt:.1f}s | MAE={metrics['MAE']:.3f}\")\n",
    "    print(f\"Epoch {epoch:02d} | train_loss={tr_loss:.4f} | \"\n",
    "          f\"MAE={metrics['MAE']:.3f} | RMSE={metrics['RMSE']:.3f} | \"\n",
    "          f\"W2={metrics['Within_2(%)']:.1f}% | F1tol2={metrics['F1_tol±2']:.3f}\")\n",
    "    if metrics[\"MAE\"] < best_mae:\n",
    "        best_mae = metrics[\"MAE\"]\n",
    "        torch.save(model.state_dict(), best_path)\n",
    "\n",
    "print(\"Best MAE:\", best_mae, \"saved:\", best_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b02eae8f",
   "metadata": {},
   "source": [
    "> ## 🔎 Lecture des métriques\n",
    ">\n",
    "> **MAE (Mean Absolute Error) ≈ 8.8 ans**\n",
    ">  → En moyenne, ton modèle se trompe de \\~9 ans.\n",
    ">  Pour une tâche d’estimation d’âge, c’est **correct** si ton dataset couvre 0–100 ans, mais ce n’est pas encore “très bon”.\n",
    ">  (En recherche, des modèles sur gros datasets arrivent plutôt entre 3–5 ans de MAE, mais avec beaucoup plus de données et souvent du pré-entrainement spécifique au visage.)\n",
    ">\n",
    ">* **RMSE ≈ 10.5 ans**\n",
    ">  → Les grosses erreurs pèsent plus (carré). Ton modèle fait parfois des erreurs bien plus grandes que 9 ans.\n",
    ">\n",
    ">* **Within 2 ans (W2) = 12.7%**\n",
    ">  → Seulement 12.7% des prédictions sont à ±2 ans de l’âge réel.\n",
    ">  C’est assez faible. Sur des modèles plus spécialisés visage, on viserait plutôt 30–40% ou plus.\n",
    ">\n",
    ">* **F1 tol ±2 = 0.225**\n",
    ">  → C’est cohérent avec W2 : ton rappel et ta précision dans cette tolérance restent faibles.\n",
    ">\n",
    ">* **Temps par époque**\n",
    ">\n",
    ">  * Epoch 1 : 1967 s (≈ 33 min) → probablement DataLoader lent au démarrage.\n",
    ">  * Epoch 2 : 2377 s (≈ 40 min).\n",
    ">  * Epoch 3 : 916 s (≈ 15 min).\n",
    ">\n",
    "> ## ⚖️ Interprétion\n",
    ">\n",
    ">* Ton modèle **apprend bien** (MAE passe de 9.58 → 8.79 en 3 époques).\n",
    ">* **Mais la performance reste moyenne** (erreur \\~9 ans).\n",
    ">* Le temps par époque est **élevé** mais acceptable pour ce volume (\\~40k images).\n",
    ">\n",
    ">## 🚀 Pistes d’amélioration\n",
    ">\n",
    ">1. **Modèle (backbone)**\n",
    ">\n",
    ">   * On a testé `convnext_tiny`.\n",
    ">   * Essayer `resnet50`, `efficientnet_b3`, ou même `swin_tiny` via `timm` : certains sont meilleurs sur visages.\n",
    ">\n",
    ">2. **Formulation de la tâche**\n",
    ">\n",
    ">   * On es en **régression directe** → souvent moins robuste.\n",
    ">   * Essayer la **classification douce (0–100 ans, soft labels)** : ça stabilise et améliore généralement MAE et W2.\n",
    ">\n",
    ">3. **Data augmentation**\n",
    ">\n",
    ">   * Améliorer la robustesse : flips, légère rotation, color jitter.\n",
    ">   * On avait mis RandomErasing : ça peut être violent sur des visages → à tester prudemment.\n",
    ">\n",
    ">4. **Entraînement plus long**\n",
    ">\n",
    ">   * Avec seulement 3 époques, on n’a pas encore convergé.\n",
    ">   * Tester 15–20 époques (en surveillant MAE val).\n",
    ">\n",
    ">5. **Batch normalization et dropout**\n",
    ">\n",
    ">   * Vérifier si on a assez de régularisation.\n",
    ">   * Un head un peu plus profond (2–3 couches) peut aider.\n",
    ">\n",
    ">**Conclusion**\n",
    ">\n",
    "> *“Après 3 époques, notre modèle ConvNeXt Tiny atteint une MAE de \\~8.8 ans. Cela montre que le modèle apprend (MAE en baisse), mais les performances restent moyennes : seule \\~12% des prédictions sont dans une tolérance de ±2 ans. Il y a encore une marge d’amélioration, notamment en testant la formulation classification douce, un backbone plus puissant (EfficientNet/ResNet50), et un entraînement plus long (10–20 époques).”*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "003d8497",
   "metadata": {},
   "source": [
    "7) Inférence sur le test + export CSV/JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966e79b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  filename  age\n",
      "0    0.JPG   32\n",
      "1    1.JPG   33\n",
      "2   10.JPG   40\n",
      "3  100.JPG   43\n",
      "4  101.JPG   40\n"
     ]
    }
   ],
   "source": [
    "# charge meilleurs poids\n",
    "model = AgeRegressor(\"convnext_tiny\", use_sex=True).to(device)\n",
    "model.load_state_dict(torch.load(\"best_model.pt\", map_location=device))\n",
    "model.eval()\n",
    "\n",
    "test_tf = val_tf  # pas d'augmentation forte à test\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self, img_dir, items, transform=None):\n",
    "        self.img_dir = Path(img_dir); self.items = items; self.transform = transform\n",
    "    def __len__(self): return len(self.items)\n",
    "    def __getitem__(self, i):\n",
    "        p = self.items[i][\"path\"]\n",
    "        img = Image.open(self.img_dir / p).convert(\"RGB\")\n",
    "        if self.transform: img = self.transform(img)\n",
    "        # sexe inconnu en test → on met 0.5 (neutre) ou 0 si non utilisé\n",
    "        sex = torch.tensor([0.5], dtype=torch.float32)\n",
    "        return img, sex, p\n",
    "\n",
    "test_ds = TestDataset(test_dir, test_items, transform=test_tf)\n",
    "test_loader = DataLoader(test_ds, batch_size=64, shuffle=False, num_workers=0, pin_memory=True)\n",
    "\n",
    "preds, names = [], []\n",
    "with torch.no_grad():\n",
    "    for imgs, sex, ps in test_loader:\n",
    "        imgs, sex = imgs.to(device), sex.to(device)\n",
    "        y = model(imgs, sex).clamp(0, 100)\n",
    "        preds.extend(y.squeeze(1).cpu().numpy().tolist())\n",
    "        names.extend(ps)\n",
    "\n",
    "# si besoin d’entiers:\n",
    "preds_int = [int(round(x)) for x in preds]\n",
    "\n",
    "# Format d’export recommandé:\n",
    "# CSV avec colonnes: filename, age\n",
    "sub_df = pd.DataFrame({\"filename\": names, \"age\": preds_int})\n",
    "sub_df.to_csv(\"submission_age.csv\", index=False)\n",
    "\n",
    "# JSON possible\n",
    "sub_df.to_json(\"submission_age.json\", orient=\"records\", lines=False)\n",
    "print(sub_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa7f5148",
   "metadata": {},
   "source": [
    "8) Évaluation robustesse (corruptions rapides)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9cf5f21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE sous bright+50%: 10.586\n"
     ]
    }
   ],
   "source": [
    "from torchvision.transforms.functional import adjust_brightness\n",
    "@torch.no_grad()\n",
    "def eval_brightness(model, loader, device, factor=1.5):\n",
    "    model.eval()\n",
    "    mae_sum, n = 0.0, 0\n",
    "    for imgs, ages, sex, _ in loader:\n",
    "        imgs_b = torch.stack([adjust_brightness(img, factor) for img in imgs])\n",
    "        imgs_b, ages, sex = imgs_b.to(device), ages.to(device), sex.to(device)\n",
    "        y = model(imgs_b, sex).clamp(0,100)\n",
    "        mae_sum += torch.abs(y-ages).sum().item()\n",
    "        n += imgs.size(0)\n",
    "    return mae_sum/n\n",
    "\n",
    "b_mae = eval_brightness(model, val_loader, device, factor=1.5)\n",
    "print(\"MAE sous bright+50%:\", round(b_mae,3))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
